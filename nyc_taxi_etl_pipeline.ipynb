{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778e901f-cc04-465c-bc89-dfafdee39da2",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385cb60a-7ced-4e0f-9129-f27034eda67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cc6bc-d157-4050-ad0d-e14a6b5fa903",
   "metadata": {},
   "source": [
    "## connecting to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3391ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'localhost'\n",
    "USER = 'root'\n",
    "PASSWORD = '01142969595'\n",
    "DATABASE = 'etl_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a81aeab9-9e18-42a6-9bbd-5a8d78f02c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"mysql+pymysql://root:01142969595@localhost/etl_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf82c8d-e7e2-4a46-9b3b-2a059cc9dfcf",
   "metadata": {},
   "source": [
    "## loading dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481619f-7572-4431-8270-ed029050bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_type_dimension = pd.read_excel(r\"E:\\data sets\\NYC_Taxi_Trips\\dimensions1.xlsx\",sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51db27-42e7-4391-a915-cc8f1ef6ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"E:/data sets/NYC_Taxi_Trips\" + \"/\" + \"dimensions1.xlsx\")\n",
    "                  #Create a list which consists of all sheet names in a Excel file.\n",
    "sheets = []\n",
    "sheets = xls.sheet_names # getting sheet names\n",
    "ex_op = open(\"E:/data sets/NYC_Taxi_Trips\" +\"/\" + \"dimensions1.xlsx\", 'rb')\n",
    "for i in sheets:\n",
    "    # Passing the sheet names as table names.\n",
    "    table_name = i\n",
    "    #read that sheet that is being processed\n",
    "    dff = pd.read_excel(ex_op, sheet_name=i)\n",
    "    # Defaulting null values to 0 to be confirmed.\n",
    "    dff=dff.fillna(0)\n",
    "    #Droping and recreating the table and inserting the data.\n",
    "    dff.to_sql(con=engine, name=table_name, if_exists='replace', schema=None,index=False)\n",
    "    # Close the Excel file.\n",
    "ex_op.close()\n",
    "print('data has been loaded into DB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d7c02-b2a2-49b4-91c1-b9e6063cb185",
   "metadata": {},
   "source": [
    "### modifing files to have separate datetime columns (step_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7366a2de-4564-4612-abed-bf02696e2f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 2017_taxi_trips.csv has been modified.\n",
      "File 2018_taxi_trips.csv has been modified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20114\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3508: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 2019_taxi_trips.csv has been modified.\n",
      "File 2020_taxi_trips.csv has been modified.\n",
      "All Files have been modified and saved in the folder.\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder path containing the files\n",
    "#folder_path = 'E:/data sets/NYC_Taxi_Trips/files to be chunked'\n",
    "folder_path = 'E:/data sets/NYC_Taxi_Trips/files to be chunked'\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "        \n",
    "        # Read the file into a DataFrame\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        dff = pd.read_csv(file_path)  # Assuming files are CSV, adjust as needed\n",
    "        \n",
    "        # Modify the DataFrame as needed\n",
    "        dff['pickup_Date'] = pd.to_datetime(dff['lpep_pickup_datetime']).dt.date # creating new column to show me the dates of the pickup\n",
    "        dff['pickup_Time'] = pd.to_datetime(dff['lpep_pickup_datetime']).dt.time # creating new column to show me the time of the pickup\n",
    "        dff['pickup_month'] = pd.to_datetime(dff['lpep_pickup_datetime']).dt.month # creating new column to show me the month of the pickup\n",
    "        dff['pickup_year'] = pd.to_datetime(dff['lpep_pickup_datetime']).dt.year # creating new column to show me the month of the pickup\n",
    "        \n",
    "        dff['dropoff_Date'] = pd.to_datetime(dff['lpep_dropoff_datetime']).dt.date # creating new column to show me the dates of the pickup\n",
    "        dff['dropoff_Time'] = pd.to_datetime(dff['lpep_dropoff_datetime']).dt.time # creating new column to show me the time of the pickup\n",
    "        dff['dropoff_month'] = pd.to_datetime(dff['lpep_dropoff_datetime']).dt.month # creating new column to show me the month of the pickup\n",
    "        dff['dropoff_year'] = pd.to_datetime(dff['lpep_dropoff_datetime']).dt.year # creating new column to show me the month of the pickup\n",
    "        dff=dff.drop(columns=\"congestion_surcharge\",errors=\"ignore\")\n",
    "        # Save the modified DataFrame back to the original file\n",
    "        dff.to_csv(file_path, index=False)  # This will overwrite the original file\n",
    "        \n",
    "        \n",
    "        print(\"File\", filename, \"has been modified.\")\n",
    "\n",
    "print(\"All Files have been modified and saved in the folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370bd920-eef1-41a6-b437-7d5677b31306",
   "metadata": {},
   "source": [
    "# Data Quality Checks (step_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92ec5f3e-a390-494a-8b08-f5272ac671a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data qulaity check 1: number of rows & columns in 2017_taxi_trips.csv are \n",
      "\n",
      "Rows: 11,740,526\n",
      "Columns: 26\n",
      "- - - - - - - - - - - - - - - -\n",
      "data qulaity check 2:  column with nulls in 2017_taxi_trips.csv are \n",
      "\n",
      "\n",
      "\n",
      "____________________________________________________________________________\n",
      "data qulaity check 1: number of rows & columns in 2018_taxi_trips.csv are \n",
      "\n",
      "Rows: 8,806,665\n",
      "Columns: 26\n",
      "- - - - - - - - - - - - - - - -\n",
      "data qulaity check 2:  column with nulls in 2018_taxi_trips.csv are \n",
      "\n",
      "\n",
      "\n",
      "____________________________________________________________________________\n",
      "data qulaity check 1: number of rows & columns in 2019_taxi_trips.csv are \n",
      "\n",
      "Rows: 5,629,292\n",
      "Columns: 26\n",
      "- - - - - - - - - - - - - - - -\n",
      "data qulaity check 2:  column with nulls in 2019_taxi_trips.csv are \n",
      "\n",
      "\n",
      "\n",
      "____________________________________________________________________________\n",
      "data qulaity check 1: number of rows & columns in 2020_taxi_trips.csv are \n",
      "\n",
      "Rows: 1,205,903\n",
      "Columns: 26\n",
      "- - - - - - - - - - - - - - - -\n",
      "data qulaity check 2:  column with nulls in 2020_taxi_trips.csv are \n",
      "\n",
      "\n",
      "\n",
      "____________________________________________________________________________\n",
      "         3 function calls in 0.001 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing CSV files\n",
    "folder_path = 'E:/data sets/NYC_Taxi_Trips/files to be chunked'\n",
    "\n",
    "def data_quality_checks(folder_path):\n",
    "# Loop through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "            \n",
    "            #counting columns\n",
    "            print(f\"data qulaity check 1: number of rows & columns in {file_name} are \\n\")\n",
    "            print(\"Rows: {:,}\".format(df.shape[0]))\n",
    "            print(df.shape[1])  \n",
    "            print(\"- - - - - - - - - - - - - - - -\")\n",
    "            #nulls in total_amount_column\n",
    "            print(f\"data qulaity check 2:  column with nulls in {file_name} are \\n\")\n",
    "            null_counts = df.isnull().sum()\n",
    "            columns_with_null = null_counts[null_counts > 0]\n",
    "            for column, count in columns_with_null.items():\n",
    "                print(f\"{column}: {count} item\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        print(\"____________________________________________________________________________\")\n",
    "data_quality_checks(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204c771-e7e0-4575-859c-5cac8cd65426",
   "metadata": {},
   "source": [
    "### extracting incorrect dates , duplicated rows and then removing it from the data set (step_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a741814-c57d-43ef-8f3a-d3ef9a86a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20114\\AppData\\Local\\Temp\\ipykernel_11204\\1794520725.py:82: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  duplicates_nulls_and_false_year(folder_path, target_folder, datetime_column,null_column)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are moved and checks are done successfully\n"
     ]
    }
   ],
   "source": [
    "def duplicates_nulls_and_false_year(folder_path, target_folder, datetime_column,null_column):\n",
    "    \"\"\"\n",
    "    Filter and drop rows from CSV files in the specified folder based on the year extracted from the filenames.\n",
    "    Save dropped data from each file into a single CSV file with an additional column for the source file name.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing CSV files.\n",
    "    - target_folder (str): Path to the folder where the dropped data CSV file will be saved.\n",
    "    - datetime_column (str): Name of the datetime column.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create target folder if it does not exist\n",
    "        if not os.path.exists(target_folder):\n",
    "            os.makedirs(target_folder)\n",
    "        \n",
    "        # Initialize DataFrame to store dropped data\n",
    "        dropped_data = pd.DataFrame()\n",
    "        duplicated_data = pd.DataFrame()\n",
    "        null_values = pd.DataFrame()\n",
    "        # Iterate through CSV files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # Extract year from filename\n",
    "                year = int(filename.split(\"_\")[0])\n",
    "                \n",
    "                # Read CSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Extract year from datetime column of the current file\n",
    "                df[datetime_column] = pd.to_datetime(df[datetime_column])\n",
    "                \n",
    "                # Filter and drop rows based on the extracted year\n",
    "                filtered_df = df[df[datetime_column].dt.year < year]\n",
    "                df = df.drop(filtered_df.index)\n",
    "                # Filter and drop rows based on duplicates\n",
    "                duplicated_df = df[df.duplicated()]\n",
    "                df = df.drop(duplicated_df.index)\n",
    "                # filter for null values\n",
    "                null_values_df = df[df[null_column].isnull()]\n",
    "                df = df.drop(null_values_df.index)\n",
    "                # Add date and time of extraction to filenames\n",
    "                current_datetime = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "                dropped_filename = f\"dropped_data_{current_datetime}.csv\"\n",
    "                duplicated_filename = f\"duplicated_rows_{current_datetime}.csv\"\n",
    "                null_df_filename = f\"null_values{current_datetime}.csv\"\n",
    "                \n",
    "                # Save filtered DataFrame back to CSV\n",
    "                df.to_csv(file_path, index=False)\n",
    "                # Save dropped and duplicated data to separate CSV files\n",
    "                dropped_data = dropped_data.append(filtered_df)\n",
    "                duplicated_data = duplicated_data.append(duplicated_df)\n",
    "                null_values = null_values.append(null_values_df)\n",
    "                \n",
    "        # Add a date and time column to dropped data\n",
    "        dropped_data['extraction_datetime'] = datetime.now()\n",
    "        duplicated_data['extraction_datetime'] = datetime.now()\n",
    "        null_values['extraction_datetime'] = datetime.now()\n",
    "\n",
    "        # Save dropped and duplicated data to separate CSV files with datetime in their names\n",
    "        dropped_data.to_csv(os.path.join(target_folder, dropped_filename), index=False)\n",
    "        duplicated_data.to_csv(os.path.join(target_folder, duplicated_filename), index=False)\n",
    "        null_values.to_csv(os.path.join(target_folder, null_df_filename), index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error filtering and dropping data: {e}\")\n",
    "    print(\"Files are moved and checks are done successfully\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'E:\\\\data sets\\\\NYC_Taxi_Trips\\\\files to be chunked'\n",
    "target_folder = \"E:\\\\data sets\\\\NYC_Taxi_Trips\\\\data quality checks\"\n",
    "datetime_column = \"lpep_pickup_datetime\"\n",
    "null_column= \"trip_type\"\n",
    "duplicates_nulls_and_false_year(folder_path, target_folder, datetime_column,null_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15fe4c6-8667-40b7-8253-4d6d48cc10e9",
   "metadata": {},
   "source": [
    "# splitting files into file chunks (month-year file name) (step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec95806a-51a5-4eb0-a461-7f9198c2f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Files have been modified and saved in the folder.\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing CSV files\n",
    "folder_path = 'E:/data sets/NYC_Taxi_Trips/files to be chunked'\n",
    "PROCESSED_FOLDER = 'E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh'\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Load the CSV file into a DataFrame\n",
    "        dff = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "\n",
    "        # Group the DataFrame by year and month\n",
    "        grouped = dff.groupby(['pickup_month', 'pickup_year'])\n",
    "\n",
    "        # Iterate over each group and save to a separate CSV file\n",
    "        for (year, month), group_dff in grouped:\n",
    "            # Generate file name with month and year\n",
    "            new_file_name = f\"{year}_{month:02d}_processing_data.csv\"\n",
    "\n",
    "            # Drop the 'year' and 'month' columns before saving\n",
    "            group_dff.drop(['pickup_year', 'pickup_month'], axis=1, inplace=True)\n",
    "\n",
    "            # Save the group to CSV file\n",
    "            group_dff.to_csv(os.path.join(PROCESSED_FOLDER, new_file_name), index=False)\n",
    "print(\"All Files have been modified and saved in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5cbf968-41ac-41b5-8751-711c6ba6526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted File Names:\n",
      "1_2017_processing_data.csv\n",
      "1_2018_processing_data.csv\n",
      "1_2019_processing_data.csv\n",
      "1_2020_processing_data.csv\n",
      "1_2021_processing_data.csv\n",
      "10_2017_processing_data.csv\n",
      "10_2018_processing_data.csv\n",
      "10_2019_processing_data.csv\n",
      "10_2020_processing_data.csv\n",
      "11_2017_processing_data.csv\n",
      "11_2018_processing_data.csv\n",
      "11_2019_processing_data.csv\n",
      "11_2020_processing_data.csv\n",
      "12_2017_processing_data.csv\n",
      "12_2018_processing_data.csv\n",
      "12_2019_processing_data.csv\n",
      "12_2020_processing_data.csv\n",
      "2_2017_processing_data.csv\n",
      "2_2018_processing_data.csv\n",
      "2_2019_processing_data.csv\n",
      "2_2020_processing_data.csv\n",
      "3_2017_processing_data.csv\n",
      "3_2018_processing_data.csv\n",
      "3_2019_processing_data.csv\n",
      "3_2020_processing_data.csv\n",
      "4_2017_processing_data.csv\n",
      "4_2018_processing_data.csv\n",
      "4_2019_processing_data.csv\n",
      "4_2020_processing_data.csv\n",
      "5_2017_processing_data.csv\n",
      "5_2018_processing_data.csv\n",
      "5_2019_processing_data.csv\n",
      "5_2020_processing_data.csv\n",
      "6_2017_processing_data.csv\n",
      "6_2018_processing_data.csv\n",
      "6_2019_processing_data.csv\n",
      "6_2020_processing_data.csv\n",
      "7_2017_processing_data.csv\n",
      "7_2018_processing_data.csv\n",
      "7_2019_processing_data.csv\n",
      "7_2020_processing_data.csv\n",
      "8_2017_processing_data.csv\n",
      "8_2018_processing_data.csv\n",
      "8_2019_processing_data.csv\n",
      "8_2020_processing_data.csv\n",
      "9_2017_processing_data.csv\n",
      "9_2018_processing_data.csv\n",
      "9_2019_processing_data.csv\n",
      "9_2020_processing_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def custom_sort(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    return (parts[0], parts[1])\n",
    "\n",
    "def get_sorted_file_names(folder_path):\n",
    "    # Get a list of file names\n",
    "    file_names = os.listdir(folder_path)\n",
    "    \n",
    "    # Sort the file names using the custom sorting function\n",
    "    sorted_file_names = sorted(file_names, key=custom_sort)\n",
    "    \n",
    "    return sorted_file_names\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh'\n",
    "sorted_files = get_sorted_file_names(folder_path)\n",
    "print(\"Sorted File Names:\")\n",
    "for file_name in sorted_files:\n",
    "    print(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ed25d-5784-4676-88d5-bcaecdc3aee5",
   "metadata": {},
   "source": [
    "# mysql fact_table creation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c761eb-5e9f-4d6f-8520-169f42f4b723",
   "metadata": {},
   "source": [
    "## create mysql fact table (step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3430417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\10_2017_processing_data.csv into nyc_trips\n",
      "Moved 10_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\10_2018_processing_data.csv into nyc_trips\n",
      "Moved 10_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\10_2019_processing_data.csv into nyc_trips\n",
      "Moved 10_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\10_2020_processing_data.csv into nyc_trips\n",
      "Moved 10_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\11_2017_processing_data.csv into nyc_trips\n",
      "Moved 11_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\11_2018_processing_data.csv into nyc_trips\n",
      "Moved 11_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\11_2019_processing_data.csv into nyc_trips\n",
      "Moved 11_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\11_2020_processing_data.csv into nyc_trips\n",
      "Moved 11_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\12_2017_processing_data.csv into nyc_trips\n",
      "Moved 12_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\12_2018_processing_data.csv into nyc_trips\n",
      "Moved 12_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\12_2019_processing_data.csv into nyc_trips\n",
      "Moved 12_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\12_2020_processing_data.csv into nyc_trips\n",
      "Moved 12_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\1_2017_processing_data.csv into nyc_trips\n",
      "Moved 1_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\1_2018_processing_data.csv into nyc_trips\n",
      "Moved 1_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\1_2019_processing_data.csv into nyc_trips\n",
      "Moved 1_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\1_2020_processing_data.csv into nyc_trips\n",
      "Moved 1_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\1_2021_processing_data.csv into nyc_trips\n",
      "Moved 1_2021_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\2_2017_processing_data.csv into nyc_trips\n",
      "Moved 2_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\2_2018_processing_data.csv into nyc_trips\n",
      "Moved 2_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\2_2019_processing_data.csv into nyc_trips\n",
      "Moved 2_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\2_2020_processing_data.csv into nyc_trips\n",
      "Moved 2_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\3_2017_processing_data.csv into nyc_trips\n",
      "Moved 3_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\3_2018_processing_data.csv into nyc_trips\n",
      "Moved 3_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\3_2019_processing_data.csv into nyc_trips\n",
      "Moved 3_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\3_2020_processing_data.csv into nyc_trips\n",
      "Moved 3_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\4_2017_processing_data.csv into nyc_trips\n",
      "Moved 4_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\4_2018_processing_data.csv into nyc_trips\n",
      "Moved 4_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\4_2019_processing_data.csv into nyc_trips\n",
      "Moved 4_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\4_2020_processing_data.csv into nyc_trips\n",
      "Moved 4_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\5_2017_processing_data.csv into nyc_trips\n",
      "Moved 5_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\5_2018_processing_data.csv into nyc_trips\n",
      "Moved 5_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\5_2019_processing_data.csv into nyc_trips\n",
      "Moved 5_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\5_2020_processing_data.csv into nyc_trips\n",
      "Moved 5_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\6_2017_processing_data.csv into nyc_trips\n",
      "Moved 6_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\6_2018_processing_data.csv into nyc_trips\n",
      "Moved 6_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\6_2019_processing_data.csv into nyc_trips\n",
      "Moved 6_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\6_2020_processing_data.csv into nyc_trips\n",
      "Moved 6_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\7_2017_processing_data.csv into nyc_trips\n",
      "Moved 7_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\7_2018_processing_data.csv into nyc_trips\n",
      "Moved 7_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\7_2019_processing_data.csv into nyc_trips\n",
      "Moved 7_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\7_2020_processing_data.csv into nyc_trips\n",
      "Moved 7_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\8_2017_processing_data.csv into nyc_trips\n",
      "Moved 8_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\8_2018_processing_data.csv into nyc_trips\n",
      "Moved 8_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\8_2019_processing_data.csv into nyc_trips\n",
      "Moved 8_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\8_2020_processing_data.csv into nyc_trips\n",
      "Moved 8_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\9_2017_processing_data.csv into nyc_trips\n",
      "Moved 9_2017_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\9_2018_processing_data.csv into nyc_trips\n",
      "Moved 9_2018_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\9_2019_processing_data.csv into nyc_trips\n",
      "Moved 9_2019_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh\\9_2020_processing_data.csv into nyc_trips\n",
      "Moved 9_2020_processing_data.csv to E:/data sets/NYC_Taxi_Trips/files uploaded to dwh\n",
      "All CSV files loaded into MySQL successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import shutil\n",
    "\n",
    "# Directory containing CSV files will be uploaded to DB\n",
    "CSV_FOLDER = 'E:/data sets/NYC_Taxi_Trips/staging file before loading data into dwh'\n",
    "# Directory to move processed CSV files\n",
    "files_uploaded_to_database = 'E:/data sets/NYC_Taxi_Trips/files uploaded to dwh'\n",
    "FACT_TABLE = 'nyc_trips'\n",
    "\n",
    "def load_csv_files_into_mysql():\n",
    "    try:\n",
    "        engine = create_engine('mysql+pymysql://root:01142969595@localhost/etl_test')\n",
    "        \n",
    "        # Get a sorted list of files in the CSV folder\n",
    "        files_sorted = sorted(os.listdir(CSV_FOLDER))\n",
    "                \n",
    "        for filename in files_sorted:\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(CSV_FOLDER, filename)\n",
    "                dff = pd.read_csv(file_path)\n",
    "                dff.to_sql(FACT_TABLE, engine, if_exists='replace', index=False)\n",
    "                print(f\"Inserted data from {file_path} into {FACT_TABLE}\")\n",
    "                new_filename = filename.replace(\"processing\", \"uploaded\")\n",
    "                shutil.move(file_path, os.path.join(files_uploaded_to_database, new_filename))\n",
    "                print(f\"Moved {filename} to {files_uploaded_to_database}\")\n",
    "        \n",
    "        print(\"All CSV files loaded into MySQL successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV files into MySQL: {e}\")\n",
    "\n",
    "# Call the function to load CSV files into MySQL\n",
    "load_csv_files_into_mysql()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e854a-1ae7-4baf-aed5-a2eba4b4dbfb",
   "metadata": {},
   "source": [
    "# mysql data_quality creation and loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15ed178a-9d6d-4606-b9d2-32cbbf810379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the CREATE TABLE query\n",
    "create_table_query = \"\"\"CREATE TABLE if not exists nyc_trips (VendorID INT, lpep_pickup_datetime VARCHAR(30), lpep_dropoff_datetime VARCHAR(30)\n",
    ", store_and_fwd_flag VARCHAR(1), RatecodeID INT\n",
    ", PULocationID INT,DOLocationID INT\n",
    ", passenger_count INT, trip_distance float, fare_amount INT, extra INT, mta_tax float, tip_amount INT, tolls_amount INT, improvement_surcharge float\n",
    ", total_amount float, payment_type INT\n",
    ", trip_type INT, pickup_Date VARCHAR(30)\n",
    ", pickup_Time Time, pickup_month INT, pickup_year INT, dropoff_Date VARCHAR(30), dropoff_Time Time, dropoff_month INT, dropoff_year INT)\"\"\"\n",
    "\n",
    "# Execute the CREATE TABLE query\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(create_table_query)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691f0244-44b3-476e-90b2-34df753a9394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted data from E:/data sets/NYC_Taxi_Trips/data quality checks\\dropped_data_20240407003604.csv into data_quality\n",
      "Moved dropped_data_20240407003604.csv to E:/data sets/NYC_Taxi_Trips/data quality checks uploaded to dwh\n",
      "Inserted data from E:/data sets/NYC_Taxi_Trips/data quality checks\\duplicated_rows_20240407003604.csv into data_quality\n",
      "Moved duplicated_rows_20240407003604.csv to E:/data sets/NYC_Taxi_Trips/data quality checks uploaded to dwh\n",
      "All CSV files loaded into MySQL successfully!\n"
     ]
    }
   ],
   "source": [
    "# Directory containing CSV files will be uploaded to DB\n",
    "DATA_QUALITY_CSV_FOLDER = 'E:/data sets/NYC_Taxi_Trips/data quality checks'\n",
    "# Directory to move processed CSV files\n",
    "files_uploaded_to_database = 'E:/data sets/NYC_Taxi_Trips/data quality checks uploaded to dwh'\n",
    "DATA_QUALITY_TABLE = 'data_quality'\n",
    "def load_data_quality_csv_files_into_mysql():\n",
    "    try:\n",
    "        engine = create_engine('mysql+pymysql://root:01142969595@localhost/etl_test')\n",
    "        files = os.listdir(DATA_QUALITY_CSV_FOLDER)\n",
    "\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(DATA_QUALITY_CSV_FOLDER, filename)\n",
    "                dff = pd.read_csv(file_path)\n",
    "                dff.to_sql(DATA_QUALITY_TABLE, engine, if_exists='append', index=False)\n",
    "                print(f\"Inserted data from {file_path} into {DATA_QUALITY_TABLE}\")\n",
    "                new_filename = filename[:-4] + \"_processed.csv\"\n",
    "                shutil.move(file_path, os.path.join(files_uploaded_to_database, new_filename))\n",
    "                print(f\"Moved {filename} to {files_uploaded_to_database}\")\n",
    "        \n",
    "        print(\"All CSV files loaded into MySQL successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV files into MySQL: {e}\")\n",
    "\n",
    "# Call the function to load CSV files into MySQL\n",
    "load_data_quality_csv_files_into_mysql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea05da3-a90f-4616-830b-8dc426fe6dc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# modifying data types from csv to mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25e75f35-1163-4ecd-8270-554df4726c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_str = \", \".join(\"{} {}\".format(n,d)for(n,d) in zip(dq.columns,dq.dtypes.replace(replacements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5906e705-5cd7-4d94-9d8d-5ae2185d8d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VendorID INT, lpep_pickup_datetime VARCHAR, lpep_dropoff_datetime VARCHAR, store_and_fwd_flag VARCHAR, RatecodeID INT, PULocationID INT, DOLocationID INT, passenger_count INT, trip_distance float, fare_amount INT, extra INT, mta_tax float, tip_amount INT, tolls_amount INT, improvement_surcharge float, total_amount float, payment_type INT, trip_type INT, pickup_Date VARCHAR, pickup_Time VARCHAR, pickup_month INT, pickup_year INT, dropoff_Date VARCHAR, dropoff_Time VARCHAR, dropoff_month INT, dropoff_year INT'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eef168e-ffb9-43e1-a48d-0cd5ed67cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20114\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3508: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "dq=pd.read_csv(r\"E:/data sets/NYC_Taxi_Trips/files to be chunked/2019_taxi_trips.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86965b1b-698f-4cc7-9217-b569c63e76c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime',\n",
       "       'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID',\n",
       "       'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax',\n",
       "       'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
       "       'payment_type', 'trip_type', 'pickup_Date', 'pickup_Time',\n",
       "       'pickup_month', 'pickup_year', 'dropoff_Date', 'dropoff_Time',\n",
       "       'dropoff_month', 'dropoff_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5425840-cb3c-4513-8a8c-f32f1d1549ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
